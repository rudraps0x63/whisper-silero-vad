{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import (\n",
    "    TYPE_CHECKING,\n",
    "    Any,\n",
    "    Callable,\n",
    "    Dict,\n",
    "    Iterator,\n",
    "    List,\n",
    "    Optional,\n",
    "    Sequence,\n",
    "    Tuple,\n",
    "    Union,\n",
    ")\n",
    "from tvm.relax.frontend import nn\n",
    "from tvm.relax.frontend.nn import Tensor, op\n",
    "from tvm.runtime import Device, NDArray, load_static_library, ndarray\n",
    "from mlc_chat.model.whisper_tiny.whisper_model import WhisperConfig, WhisperForConditionalGeneration\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tvm sucessfully installed\n",
      "mlc_chat sucessfully installed\n"
     ]
    }
   ],
   "source": [
    "!python -c \"import tvm; print('tvm sucessfully installed')\"\n",
    "!python -c \"import mlc_chat; print('mlc_chat sucessfully installed')\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For compiling, weight, config generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vulkan device found\n"
     ]
    }
   ],
   "source": [
    "import tvm\n",
    "device_str = \"vulkan\"\n",
    "device = tvm.runtime.device(device_str)\n",
    "if device.exist:\n",
    "    print(f\"{device_str} device found\")\n",
    "else:\n",
    "    print(f\"{device_str} device not found\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/munusairam/softwares/programs/python_projects/tvm-framework/source-build-support-tvm-mlc-whisper-tiny/mlc-llm/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import mlc_chat.cli.convert_weight as cv\n",
    "import mlc_chat.cli.compile as c\n",
    "import mlc_chat.cli.gen_config as gencfg\n",
    "import mlc_chat.cli.check_device as cdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mWeight conversion with arguments:\u001b[0m\n",
      "  \u001b[1m--config\u001b[0m          ../dist/models/whisper-tiny/config.json\n",
      "  \u001b[1m--quantization\u001b[0m    NoQuantize(name='q0f32', kind='no-quant', model_dtype='float32')\n",
      "  \u001b[1m--model-type\u001b[0m      whisper-tiny\n",
      "  \u001b[1m--device\u001b[0m          vulkan:0\n",
      "  \u001b[1m--source\u001b[0m          ../dist/models/whisper-tiny/pytorch_model.bin\n",
      "  \u001b[1m--source-format\u001b[0m   huggingface-torch\n",
      "  \u001b[1m--output\u001b[0m          ../dist/libs/whisper-tiny\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 168/168 [00:00<00:00, 286.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start storing to cache ../dist/libs/whisper-tiny\n",
      "[0168/0168] saving model.decoder.layer_norm.bias                        \n",
      "All finished, 4 total shards committed, record saved to ../dist/libs/whisper-tiny/ndarray-cache.json\n",
      "Also saved a bf16 record to ../dist/libs/whisper-tiny/ndarray-cache-b16.json\n"
     ]
    }
   ],
   "source": [
    "cv.main([\"--model-type\", \"whisper-tiny\", \"../dist/models/whisper-tiny/\", \"--quantization\", \"q0f32\", \"-o\", \"../dist/libs/whisper-tiny/\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gencfg.main([\"--model-type\", \"whisper-tiny\", \"../dist/models/whisper-tiny/\", \"--quantization\", \"q0f32\", \"--conv-template\", \"whisper\",  \"--output\", \"../dist/libs/whisper-tiny/\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG_DUMP = \"../dist/libs/whisper-tiny/debug_dump\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for vulkan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mCompiling with arguments:\u001b[0m\n",
      "  \u001b[1m--config\u001b[0m          WhisperConfig(vocab_size=51865, num_mel_bins=80, encoder_layers=4, encoder_attention_heads=6, decoder_layers=4, decoder_attention_heads=6, decoder_ffn_dim=1536, encoder_ffn_dim=1536, d_model=384, max_source_positions=1500, max_target_positions=448, pad_token_id=50257, context_window_size=448, prefill_chunk_size=448, tensor_parallel_shards=1, kwargs={'model_type': 'whisper-tiny', 'quantization': 'q0f32', 'model_config': {'vocab_size': 51865, 'num_mel_bins': 80, 'encoder_layers': 4, 'encoder_attention_heads': 6, 'decoder_layers': 4, 'decoder_attention_heads': 6, 'decoder_ffn_dim': 1536, 'encoder_ffn_dim': 1536, 'd_model': 384, 'max_source_positions': 1500, 'max_target_positions': 448, 'pad_token_id': 50257, 'context_window_size': 448, 'prefill_chunk_size': 448, 'tensor_parallel_shards': 1}, 'sliding_window_size': -1, 'attention_sink_size': -1, 'mean_gen_len': 128, 'max_gen_len': 512, 'shift_fill_factor': 0.3, 'temperature': 0.7, 'repetition_penalty': 1.0, 'top_p': 0.95, 'conv_template': 'whisper', 'bos_token_id': 50257, 'eos_token_id': 50257, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'forced_decoder_ids': [[1, None], [2, 50359]], 'begin_suppress_tokens': [220, 50257], 'tokenizer_files': ['tokenizer.json', 'vocab.json', 'merges.txt', 'added_tokens.json', 'tokenizer_config.json'], 'version': '0.1.0'})\n",
      "  \u001b[1m--quantization\u001b[0m    NoQuantize(name='q0f32', kind='no-quant', model_dtype='float32')\n",
      "  \u001b[1m--model-type\u001b[0m      whisper-tiny\n",
      "  \u001b[1m--target\u001b[0m          {\"thread_warp_size\": 1, \"host\": {\"mtriple\": \"x86_64-redhat-linux-gnu\", \"tag\": \"\", \"kind\": \"llvm\", \"mcpu\": \"znver3\", \"keys\": [\"cpu\"]}, \"supports_int16\": 1, \"supports_float32\": T.bool(True), \"supports_int32\": T.bool(True), \"max_threads_per_block\": 1024, \"supports_int8\": 1, \"max_num_threads\": 256, \"kind\": \"vulkan\", \"max_shared_memory_per_block\": 65536, \"supports_16bit_buffer\": 1, \"tag\": \"\", \"keys\": [\"vulkan\", \"gpu\"], \"supports_float16\": 1}\n",
      "  \u001b[1m--opt\u001b[0m             flashinfer=0;cublas_gemm=0;cudagraph=0\n",
      "  \u001b[1m--system-lib-prefix\u001b[0m \"\"\n",
      "  \u001b[1m--output\u001b[0m          ../dist/libs/whisper-tiny/whisper-tiny.so\n",
      "  \u001b[1m--overrides\u001b[0m       context_window_size=None;sliding_window_size=None;prefill_chunk_size=None;attention_sink_size=None;max_batch_size=None;tensor_parallel_shards=None\n"
     ]
    }
   ],
   "source": [
    "c.main([\"--model-type\", \"whisper-tiny\", \"../dist/libs/whisper-tiny/\",\"--quantization\", \"q0f32\", \"--device\", \"vulkan\",  \"--output\", \"../dist/libs/whisper-tiny/whisper-tiny.so\" ,\"--debug-dump\", DEBUG_DUMP])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for android"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c.main([\"--model-type\", \"whisper-tiny\", \"/home/munusairam/softwares/programs/python_projects/whisper-tiny/\",\"--quantization\", \"q4f32_1\", \"--device\", \"android\",  \"--output\", \"/home/munusairam/softwares/programs/python_projects/whisper-tiny/whisper_tiny_q4f32_1_android.tar\" ,\"--debug-dump\", DEBUG_DUMP])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. gen_config: generate mlc-chat-config.json and process tokenizers\n",
    "#!mlc_chat gen_config ./dist/models/gpt2 --quantization q4f16_1 --conv-template gpt2 \n",
    "#    -o dist/gpt2-q4f16_1-MLC/\n",
    "    \n",
    "\n",
    "# 2. compile: compile model library with specification in mlc-chat-config.json\n",
    "#!mlc_chat compile ./dist/gpt2-q4f16_1-MLC/mlc-chat-config.json \\\n",
    "#    --device cuda -o dist/gpt2-q4f16_1-MLC/gpt2-q4f16_1-cuda.so"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
