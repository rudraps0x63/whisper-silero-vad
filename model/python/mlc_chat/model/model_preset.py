"""A builtin set of models available in MLC LLM."""

from typing import Any, Dict

MODEL_PRESETS: Dict[str, Any] = {
    "llama2_7b": {
        "architectures": ["LlamaForCausalLM"],
        "bos_token_id": 1,
        "eos_token_id": 2,
        "hidden_act": "silu",
        "hidden_size": 4096,
        "initializer_range": 0.02,
        "intermediate_size": 11008,
        "max_position_embeddings": 2048,
        "model_type": "llama",
        "num_attention_heads": 32,
        "num_hidden_layers": 32,
        "num_key_value_heads": 32,
        "pad_token_id": 0,
        "pretraining_tp": 1,
        "rms_norm_eps": 1e-05,
        "rope_scaling": None,
        "tie_word_embeddings": False,
        "torch_dtype": "float16",
        "transformers_version": "4.31.0.dev0",
        "use_cache": True,
        "vocab_size": 32000,
        "context_window_size": 2048,
        "prefill_chunk_size": 2048,
    },
    "llama2_13b": {
        "_name_or_path": "meta-llama/Llama-2-13b-hf",
        "architectures": ["LlamaForCausalLM"],
        "bos_token_id": 1,
        "eos_token_id": 2,
        "hidden_act": "silu",
        "hidden_size": 5120,
        "initializer_range": 0.02,
        "intermediate_size": 13824,
        "max_position_embeddings": 2048,
        "model_type": "llama",
        "num_attention_heads": 40,
        "num_hidden_layers": 40,
        "num_key_value_heads": 40,
        "pad_token_id": 0,
        "pretraining_tp": 2,
        "rms_norm_eps": 1e-05,
        "rope_scaling": None,
        "tie_word_embeddings": False,
        "torch_dtype": "float16",
        "transformers_version": "4.31.0.dev0",
        "use_cache": True,
        "vocab_size": 32000,
        "context_window_size": 2048,
        "prefill_chunk_size": 2048,
    },
    "llama2_70b": {
        "architectures": ["LlamaForCausalLM"],
        "bos_token_id": 1,
        "eos_token_id": 2,
        "hidden_act": "silu",
        "hidden_size": 8192,
        "initializer_range": 0.02,
        "intermediate_size": 28672,
        "max_position_embeddings": 2048,
        "model_type": "llama",
        "num_attention_heads": 64,
        "num_hidden_layers": 80,
        "num_key_value_heads": 8,
        "pad_token_id": 0,
        "rms_norm_eps": 1e-05,
        "tie_word_embeddings": False,
        "torch_dtype": "float16",
        "transformers_version": "4.31.0.dev0",
        "use_cache": True,
        "vocab_size": 32000,
        "context_window_size": 2048,
        "prefill_chunk_size": 2048,
    },
    "codellama_7b": {
        "_name_or_path": "codellama/CodeLlama-7b-hf",
        "architectures": ["LlamaForCausalLM"],
        "bos_token_id": 1,
        "eos_token_id": 2,
        "hidden_act": "silu",
        "hidden_size": 4096,
        "initializer_range": 0.02,
        "intermediate_size": 11008,
        "max_position_embeddings": 16384,
        "model_type": "llama",
        "num_attention_heads": 32,
        "num_hidden_layers": 32,
        "num_key_value_heads": 32,
        "pretraining_tp": 1,
        "rms_norm_eps": 1e-05,
        "rope_scaling": None,
        "rope_theta": 1000000,
        "tie_word_embeddings": False,
        "torch_dtype": "bfloat16",
        "transformers_version": "4.33.0.dev0",
        "use_cache": True,
        "vocab_size": 32016,
        "context_window_size": 2048,
        "prefill_chunk_size": 2048,
    },
    "codellama_13b": {
        "architectures": ["LlamaForCausalLM"],
        "bos_token_id": 1,
        "eos_token_id": 2,
        "hidden_act": "silu",
        "hidden_size": 5120,
        "initializer_range": 0.02,
        "intermediate_size": 13824,
        "max_position_embeddings": 16384,
        "model_type": "llama",
        "num_attention_heads": 40,
        "num_hidden_layers": 40,
        "num_key_value_heads": 40,
        "pretraining_tp": 1,
        "rms_norm_eps": 1e-05,
        "rope_scaling": None,
        "rope_theta": 1000000,
        "tie_word_embeddings": False,
        "torch_dtype": "bfloat16",
        "transformers_version": "4.32.0.dev0",
        "use_cache": True,
        "vocab_size": 32016,
        "context_window_size": 2048,
        "prefill_chunk_size": 2048,
    },
    "codellama_34b": {
        "architectures": ["LlamaForCausalLM"],
        "bos_token_id": 1,
        "eos_token_id": 2,
        "hidden_act": "silu",
        "hidden_size": 8192,
        "initializer_range": 0.02,
        "intermediate_size": 22016,
        "max_position_embeddings": 16384,
        "model_type": "llama",
        "num_attention_heads": 64,
        "num_hidden_layers": 48,
        "num_key_value_heads": 8,
        "pretraining_tp": 1,
        "rms_norm_eps": 1e-05,
        "rope_scaling": None,
        "rope_theta": 1000000,
        "tie_word_embeddings": False,
        "torch_dtype": "bfloat16",
        "transformers_version": "4.32.0.dev0",
        "use_cache": True,
        "vocab_size": 32016,
        "context_window_size": 2048,
        "prefill_chunk_size": 2048,
    },
    "mistral_7b": {
        "architectures": ["MistralForCausalLM"],
        "bos_token_id": 1,
        "eos_token_id": 2,
        "hidden_act": "silu",
        "hidden_size": 4096,
        "initializer_range": 0.02,
        "intermediate_size": 14336,
        "max_position_embeddings": 32768,
        "model_type": "mistral",
        "num_attention_heads": 32,
        "num_hidden_layers": 32,
        "num_key_value_heads": 8,
        "rms_norm_eps": 1e-05,
        "rope_theta": 10000.0,
        "tie_word_embeddings": False,
        "torch_dtype": "bfloat16",
        "transformers_version": "4.34.0.dev0",
        "use_cache": True,
        "vocab_size": 32000,
        "sliding_window_size": 4096,
        "prefill_chunk_size": 128,
        "attention_sink_size": 4,
    },
    "gpt2": {
        "architectures": ["GPT2LMHeadModel"],
        "bos_token_id": 50256,
        "eos_token_id": 50256,
        "hidden_act": "gelu_new",
        "n_embd": 768,
        "initializer_range": 0.02,
        "n_positions": 1024,
        "model_type": "gpt2",
        "n_head": 12,
        "n_layer": 12,
        "layer_norm_epsilon": 1e-05,
        "transformers_version": "4.26.0.dev0",
        "use_cache": True,
        "vocab_size": 50257,
        "context_window_size": 2048,
        "prefill_chunk_size": 2048,
    },
    "redpajama_3b_v1": {
        "_name_or_path": "/root/fm/models/rp_3b_800b_real_fp16",
        "architectures": ["GPTNeoXForCausalLM"],
        "bos_token_id": 0,
        "eos_token_id": 0,
        "hidden_act": "gelu",
        "hidden_size": 2560,
        "initializer_range": 0.02,
        "intermediate_size": 10240,
        "layer_norm_eps": 1e-05,
        "max_position_embeddings": 2048,
        "model_type": "gpt_neox",
        "num_attention_heads": 32,
        "num_hidden_layers": 32,
        "rotary_emb_base": 10000,
        "rotary_pct": 1.0,
        "tie_word_embeddings": False,
        "torch_dtype": "float16",
        "transformers_version": "4.28.1",
        "use_cache": True,
        "use_parallel_residual": False,
        "vocab_size": 50432,
    },
    "phi-1_5": {
        "_name_or_path": "microsoft/phi-1_5",
        "activation_function": "gelu_new",
        "architectures": ["PhiForCausalLM"],
        "attn_pdrop": 0.0,
        "auto_map": {
            "AutoConfig": "configuration_phi.PhiConfig",
            "AutoModelForCausalLM": "modeling_phi.PhiForCausalLM",
        },
        "embd_pdrop": 0.0,
        "flash_attn": False,
        "flash_rotary": False,
        "fused_dense": False,
        "initializer_range": 0.02,
        "layer_norm_epsilon": 1e-05,
        "model_type": "phi-msft",
        "n_embd": 2048,
        "n_head": 32,
        "n_head_kv": None,
        "n_inner": None,
        "n_layer": 24,
        "n_positions": 2048,
        "resid_pdrop": 0.0,
        "rotary_dim": 32,
        "tie_word_embeddings": False,
        "torch_dtype": "float16",
        "transformers_version": "4.34.1",
        "vocab_size": 51200,
    },
    "phi-2": {
        "_name_or_path": "microsoft/phi-2",
        "activation_function": "gelu_new",
        "architectures": ["PhiForCausalLM"],
        "attn_pdrop": 0.0,
        "auto_map": {
            "AutoConfig": "configuration_phi.PhiConfig",
            "AutoModelForCausalLM": "modeling_phi.PhiForCausalLM",
        },
        "embd_pdrop": 0.0,
        "flash_attn": False,
        "flash_rotary": False,
        "fused_dense": False,
        "img_processor": None,
        "initializer_range": 0.02,
        "layer_norm_epsilon": 1e-05,
        "model_type": "phi-msft",
        "n_embd": 2560,
        "n_head": 32,
        "n_head_kv": None,
        "n_inner": None,
        "n_layer": 32,
        "n_positions": 2048,
        "resid_pdrop": 0.1,
        "rotary_dim": 32,
        "tie_word_embeddings": False,
        "torch_dtype": "float16",
        "transformers_version": "4.35.2",
        "vocab_size": 51200,
    },
    "whisper-tiny": {
        "architectures": ["WhisperForConditionalGeneration"],
        "attention_dropout": 0.0,
        "activation_function": "gelu",
        "begin_suppress_tokens": [220, 50257],
        "bos_token_id": 50257,
        "d_model": 384,
        "decoder_attention_heads": 6,
        "decoder_ffn_dim": 1536,
        "decoder_layerdrop": 0.0,
        "decoder_layers": 4,
        "decoder_start_token_id": 50258,
        "dropout": 0.0,
        "encoder_attention_heads": 6,
        "encoder_ffn_dim": 1536,
        "encoder_layerdrop": 0.0,
        "encoder_layers": 4,
        "eos_token_id": 50257,
        "forced_decoder_ids": [[1, 50259], [2, 50359], [3, 50363]],
        "init_std": 0.02,
        "max_length": 448,
        "max_source_positions": 1500,
        "max_target_positions": 448,
        "num_hidden_layers": 4,
        "num_mel_bins": 80,
        "pad_token_id": 50257,
        "scale_embedding": False,
        "suppress_tokens": [
            1,
            2,
            7,
            8,
            9,
            10,
            14,
            25,
            26,
            27,
            28,
            29,
            31,
            58,
            59,
            60,
            61,
            62,
            63,
            90,
            91,
            92,
            93,
            359,
            503,
            522,
            542,
            873,
            893,
            902,
            918,
            922,
            931,
            1350,
            1853,
            1982,
            2460,
            2627,
            3246,
            3253,
            3268,
            3536,
            3846,
            3961,
            4183,
            4667,
            6585,
            6647,
            7273,
            9061,
            9383,
            10428,
            10929,
            11938,
            12033,
            12331,
            12562,
            13793,
            14157,
            14635,
            15265,
            15618,
            16553,
            16604,
            18362,
            18956,
            20075,
            21675,
            22520,
            26130,
            26161,
            26435,
            28279,
            29464,
            31650,
            32302,
            32470,
            36865,
            42863,
            47425,
            49870,
            50254,
            50258,
            50358,
            50359,
            50360,
            50361,
            50362,
        ],
        "use_cache": True,
        "vocab_size": 51865,
    },
    "marian": {
        "vocab_size": 59514,
        "decoder_vocab_size": 59514,
        "max_position_embeddings": 512,
        "d_model": 512,
        "encoder_ffn_dim": 2048,
        "encoder_layers": 6,
        "encoder_attention_heads": 8,
        "decoder_ffn_dim": 2048,
        "decoder_layers": 6,
        "decoder_attention_heads": 8,
        "dropout": 0.1,
        "attention_dropout": 0.0,
        "activation_dropout": 0.0,
        "activation_function": "swish",
        "init_std": 0.02,
        "encoder_layerdrop": 0.0,
        "decoder_layerdrop": 0.0,
        "use_cache": True,
        "num_hidden_layers": 6,
        "scale_embedding": True,
        "share_encoder_decoder_embeddings": True,
        "return_dict": True,
        "output_hidden_states": False,
        "output_attentions": False,
        "torchscript": False,
        "torch_dtype": None,
        "use_bfloat16": False,
        "tf_legacy_loss": False,
        "pruned_heads": {},
        "tie_word_embeddings": True,
        "is_encoder_decoder": True,
        "is_decoder": False,
        "cross_attention_hidden_size": None,
        "add_cross_attention": False,
        "tie_encoder_decoder": False,
        "max_length": 512,
        "min_length": 0,
        "do_sample": False,
        "early_stopping": False,
        "num_beams": 4,
        "num_beam_groups": 1,
        "diversity_penalty": 0.0,
        "temperature": 1.0,
        "top_k": 50,
        "top_p": 1.0,
        "typical_p": 1.0,
        "repetition_penalty": 1.0,
        "length_penalty": 1.0,
        "no_repeat_ngram_size": 0,
        "encoder_no_repeat_ngram_size": 0,
        "bad_words_ids": [[59513]],
        "num_return_sequences": 1,
        "chunk_size_feed_forward": 0,
        "output_scores": False,
        "return_dict_in_generate": False,
        "forced_bos_token_id": None,
        "forced_eos_token_id": 0,
        "remove_invalid_values": False,
        "exponential_decay_length_penalty": None,
        "suppress_tokens": None,
        "begin_suppress_tokens": None,
        "architectures": ["MarianMTModel"],
        "finetuning_task": None,
        "id2label": {0: "LABEL_0", 1: "LABEL_1", 2: "LABEL_2"},
        "label2id": {"LABEL_0": 0, "LABEL_1": 1, "LABEL_2": 2},
        "tokenizer_class": None,
        "prefix": None,
        "bos_token_id": 0,
        "pad_token_id": 59513,
        "eos_token_id": 0,
        "sep_token_id": None,
        "decoder_start_token_id": 59513,
        "task_specific_params": None,
        "problem_type": None,
        "_name_or_path": "Helsinki-NLP/opus-mt-en-fr",
        "transformers_version": "4.36.2",
        "_num_labels": 3,
        "add_bias_logits": False,
        "add_final_layer_norm": False,
        "classif_dropout": 0.0,
        "classifier_dropout": 0.0,
        "gradient_checkpointing": False,
        "model_type": "marian",
        "normalize_before": False,
        "normalize_embedding": False,
        "static_position_embeddings": True,
    },
}
